\section{Prompt Examples}
\label{app:prompts}

This appendix shows actual prompts from my Claude Code sessions. These are verbatim---I haven't cleaned them up. They illustrate the dramatic difference in interaction style across the autonomy gradient.

\subsection{High Autonomy ($\sim$95\%): Code Implementation}

High-autonomy tasks required almost no specification. This four-word prompt produced a complete, working diagnostic module:

\begin{lstlisting}[caption={Human prompt for diagnostic implementation}]
Sounds good, let's do diagnostics
\end{lstlisting}

Context: we had already discussed that energy spectrum diagnostics were needed. Four words. Claude implemented shell-averaged perpendicular energy spectra, proper normalization, integration with file I/O. No iteration required.

Similarly, addressing code review feedback took just a URL:

\begin{lstlisting}[caption={Human prompt for addressing review feedback}]
Address review comments: https://github.com/anjor/gandalf/pull/18
\end{lstlisting}

Claude read the review comments, implemented all changes, pushed updates. I verified through physics outputs, not code inspection.

\textbf{Key characteristics}: Unambiguous context, established patterns, objective success criteria.

\subsection{Medium Autonomy ($\sim$50\%): Paper Writing}

Paper writing was different. Multiple iterations, substantial editing.

\begin{lstlisting}[caption={Human prompt for paper section}]
Read issue #5 and find the thesis chapter in the repo. Extract and
adapt the KRMHD formulation for a journal paper. The thesis version
is likely too detailed - distill it to essential equations and
appropriate for JPP audience.
\end{lstlisting}

After the first draft:

\begin{lstlisting}[caption={Human feedback after reviewing draft}]
Address the feedback on https://github.com/anjor/gandalf-paper/pull/17
\end{lstlisting}

And later:

\begin{lstlisting}[caption={Human request for output verification}]
can you generate the pdf so that i can read the content
\end{lstlisting}

Multiple iterations to reach acceptable quality. Claude's drafts were technically accurate but missed on tone, emphasis, audience. I had to do real editing.

\textbf{Key characteristics}: Subjective quality, audience-dependent, requires judgment about what matters.

\subsection{Low Autonomy ($\sim$10\%): Parameter Exploration}

Parameter tuning for turbulence demonstrated the physics taste deficit most clearly. This sequence shows how much guidance I had to provide:

\begin{lstlisting}[caption={Human identifies problem with simulation output}]
Look at @examples/output/driven_energy_spectra.png -- shouldn't we
see a -5/3 spectrum here for k_perp? But we are not?
\end{lstlisting}

Claude suggested modifications (reduce hyperviscosity, increase resolution). After implementation, issues persisted:

\begin{lstlisting}[caption={Human probes for solution direction}]
can we increase the forcing? Do you think that will help? Right now
it seems like energy is not moving to larger k's quickly enough. I
am also ok with forcing k=1 and 2
\end{lstlisting}

The conversation continued with the human maintaining direction:

\begin{lstlisting}[caption={Human provides guidance based on physics intuition}]
I think we need stronger forcing
\end{lstlisting}

And requesting information to inform decisions:

\begin{lstlisting}[caption={Human gathers data for next decision}]
What's our forcing range?
\end{lstlisting}

This pattern---I identify problem, I suggest direction, I request information, I decide next step---continued for multiple sessions. Claude implemented each change correctly but didn't converge toward the solution. I found it (substantially increased forcing) through physical intuition about inertial range requirements.

\textbf{Key characteristics}: Multiple plausible interventions, requires building intuition across attempts, success depends on recognizing subtle signatures.

\subsection{Summary}

The pattern is clear: AI effectiveness correlates with specification clarity and objective success criteria. Tasks requiring intuition, audience awareness, or hypothesis refinement remain human-dependent regardless of AI coding capability.

Effective AI collaboration uses very short prompts (``Sounds good, let's do diagnostics'') when context is established, but requires more detail for new task categories. My role shifted from implementation to orchestration---deciding \emph{what} to build rather than \emph{how}.
